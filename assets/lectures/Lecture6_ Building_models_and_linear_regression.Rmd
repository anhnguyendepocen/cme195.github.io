---
title: 'Lecture 6: Hypothesis testing and Linear Regression'
date: "October 16, 2018"
output: 
  revealjs::revealjs_presentation:
    self_contained: false
    lib_dir: libs
    theme: simple
    hightlights: haddock
    smart: true
    center: true
    transition: slide
    css: cme195.css
    fig_width: 10
    fig_height: 6
    reveal_options:
      slideNumber: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
.packages <- c("ggplot2", "plotly", "plot3D")
lapply(.packages, require, character.only = TRUE)
```

## Contents



<div class="left", style="width: 50%">

* Building Models

* Linear Regression

* Lasso Regression


</div>

<div class="right", style="width: 50%">
![](./Lecture6-figure/data-science-model.png)
</div>


## Useful Books

* ["An introduction to Statistical Learning"](http://www-bcf.usc.edu/~gareth/ISL/getbook.html) [ISL] by James, Witten, Hastie and Tibshirani 
* ["Elements of statistical learning"](http://www.springer.com/gp/book/9780387848570) [ESL] by Hastie, Tibshirani and Friedman 

* ["Introduction to Linear Regression Analysis"](http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470542810.html) by Montgomery, Peck, Vinning


# Building Models

## Introduction to models
The goal of a model is to provide a simple low-dimensional summary of a dataset, which we can use to partition data into patterns and residuals.

There are two parts to a model:

define a family of models: type of pattern that you want to capture
generate a fitted model: the closest, within the family, to your data
“All models are wrong, but some are useful.” G. Cox, 1976
We will use the modelr package, which wraps around base R’s modelling functions to make them work naturally in a pipe.


library(modelr)

## Defining a family of models
Toy dataset: sim1, contained in package modelr.

ggplot(sim1, aes(x, y)) + geom_point()


Use a model to capture this pattern and make it explicit.

The relationship looks linear: y=a0+a1×x


Defining the best model
Family of models: y=a0+a1×x.

Two parameters, a0 and a1, identify the specific model.

Formulas in R
Modelling functions use a standard conversion from formulas to functions.

lm(y~x)
y=a0+a1×x

Formulas with categorical variables
df <- tribble(
  ~ sex, ~age, ~ response,
  "male", 28, 1,
  "female", 32, 2,
  "male", 26, 1
)
## # A tibble: 3 x 3
##   sex      age response
##   <chr>  <dbl>    <dbl>
## 1 male    28.0     1.00
## 2 female  32.0     2.00
## 3 male    26.0     1.00
It doesn’t make sense to convert as before, because “sex” is not a number.

Instead, R creates a column that is 1 if “male”, and 0 if “female”.

model_matrix(df, response ~ sex)
## # A tibble: 3 x 2
##   `(Intercept)` sexmale
##           <dbl>   <dbl>
## 1            1.      1.
## 2            1.      0.
## 3            1.      1.
In general, it creates k−1 columns, where k is the number of categories.

But you don’t need to worry about the parametrization to make predictions.

Formulas with interactions
In the sim3 dataset, there is a categorical and a continuous predictor.

ggplot(sim3, aes(x=x1, y=y)) + geom_point(aes(color = x2)) 


We could fit two different models

mod1 <- lm(y ~ x1 + x2, data = sim3)  # Model without interactions
mod2 <- lm(y ~ x1 * x2, data = sim3)  # Model with interactions
Models with interactions
grid <- sim3 %>% data_grid(x1, x2) %>% # data grid for two variables
  gather_predictions(mod1, mod2)       # predictions from both models

ggplot(sim3, aes(x=x1, y=y, color=x2)) +   
  geom_point() +                       # plot data points
  geom_line(data=grid, aes(y=pred)) +  # add predictions for each model
  facet_wrap(~ model)                  # show one plot for each model


The model that uses * has a different slope and intercept for each line.

Model building
Using models to understand data
We have introduced models using simulated data.

Now we discuss how to progressively build up a model for real data.

We will proceed iteratively:

find patterns with visualisation,
make them concrete and precise with a model,
repeat, replacing the old response variable with the residuals.
Knowing when to stop is not easy.


Our goal is to transition

from implicit knowledge in the data and your head
to explicit knowledge in a quantitative model.

There are more automated ways, useful for very large and complex data.

Date-time data in R


Formulas with interactions
In the sim3 dataset, there is a categorical and a continuous predictor.

ggplot(sim3, aes(x=x1, y=y)) + geom_point(aes(color = x2)) 


We could fit two different models

mod1 <- lm(y ~ x1 + x2, data = sim3)  # Model without interactions
mod2 <- lm(y ~ x1 * x2, data = sim3)  # Model with interactions

Models with interactions
grid <- sim3 %>% data_grid(x1, x2) %>% # data grid for two variables
  gather_predictions(mod1, mod2)       # predictions from both models

ggplot(sim3, aes(x=x1, y=y, color=x2)) +   
  geom_point() +                       # plot data points
  geom_line(data=grid, aes(y=pred)) +  # add predictions for each model
  facet_wrap(~ model)                  # show one plot for each model


The model that uses * has a different slope and intercept for each line.

Residuals with interactions
Which model should we choose? Let’s look at the residuals

sim3 <- sim3 %>% gather_residuals(mod1, mod2)

ggplot(sim3, aes(x=x1, y=resid, color = x2)) + 
  geom_point() + facet_grid(model ~ x2)


There is little obvious pattern in the residuals for mod2.

Interactions of continuous variables
In the sim4 dataset, there are two continuous predictors.

p1 <- ggplot(sim4, aes(x=x1, y=y)) + geom_point(aes(color = x2))
p2 <- ggplot(sim4, aes(x=x2, y=y)) + geom_point(aes(color = x1))




Interactions models can be fit in the same way as before.

mod1 <- lm(y ~ x1 + x2, data = sim4)  # Model without interactions
mod2 <- lm(y ~ x1 * x2, data = sim4)  # Model with interactions

Interaction models (II)
Left: without interactions. Right: with interactions.

Formulas with transformations
You can perform transformations inside the model formula.

If your transformation involves +, *, ^, or -, you need to wrap it in I().

Approximating non-linear functions
You can use transformations to approximate non-linear functions.

Taylor’s theorem: you can approximate any smooth function with an infinite sum of polynomials.


For example, poly() evaluates orthogonal polynomials

model_matrix(df, y ~ poly(x, degree = 2))
## # A tibble: 3 x 3
##   `(Intercept)` `poly(x, degree = 2)1` `poly(x, degree = 2)2`
##           <dbl>                  <dbl>                  <dbl>
## 1          1.00 -0.707                                  0.408
## 2          1.00 -0.0000000000000000785                 -0.816
## 3          1.00  0.707                                  0.408

Alternative: splines (piecewise polynomials), from the splines package.

Approximating non-linear functions (example)
Approximating with orthogonal polynomials of different degrees.



mod1 <- lm(y ~ poly(x, 1), data = sim5)
mod2 <- lm(y ~ poly(x, 2), data = sim5)
mod3 <- lm(y ~ poly(x, 3), data = sim5)
mod4 <- lm(y ~ poly(x, 4), data = sim5)

grid <- sim5 %>% 
  data_grid(x = seq_range(x, n = 50, expand = 0.1)) %>% 
  gather_predictions(mod1, mod2, mod3, mod4, .pred = "y")
  
  
Other model families
This chapter has focussed exclusively on the class of linear models.
y=a0+a1×x1+a2×x2+...+an×xn

There are a large set of other model classes.

Extensions of linear models:

Generalised linear models, stats::glm(), binary or count data.
Generalised additive models, mgcv::gam(), extend generalised linear models to incorporate arbitrary smooth functions.
Penalised linear models, glmnet::glmnet(), add a penalty term to the distance that penalises complex models.
Robust linear models, MASS:rlm(), less sensitive to outliers.
Completely different models:

Trees, rpart::rpart(), fit a piece-wise constant model splitting the data into progressively smaller and smaller pieces.
Random forests, randomForest::randomForest(), aggregate many different trees.
Gradient boosting machines, xgboost::xgboost, aggregate trees.

## Model Building

Using models to understand data
We have introduced models using simulated data.

Now we discuss how to progressively build up a model for real data.

We will proceed iteratively:

find patterns with visualisation,
make them concrete and precise with a model,
repeat, replacing the old response variable with the residuals.
Knowing when to stop is not easy.


Our goal is to transition

from implicit knowledge in the data and your head
to explicit knowledge in a quantitative model.

There are more automated ways, useful for very large and complex data.

he diamond dataset
We begin by working with the diamond dataset.

library(tidyverse)
library(modelr)
options(na.action = na.warn)
Why do low-quality diamonds appear more expensive?





Remember that diamond color goes from J (worst) to D (best).


Price and carat
Focus on diamonds smaller than 2.5 carats (99.7% of the data). Log-transform carat and price, then fit linear model.

diamonds2 <- diamonds %>% filter(carat <= 2.5) %>% 
  mutate(lprice = log2(price), lcarat = log2(carat))
mod_diamond <- lm(lprice ~ lcarat, data = diamonds2)
diamonds.mod <- diamonds2 %>% 
  add_predictions(mod_diamond, "lprice.pred") %>%
  add_residuals(mod_diamond, "lprice.resid")
  
  Residuals
p1 <- ggplot(diamonds.mod, aes(cut, lprice.resid)) + geom_boxplot()
p2 <- ggplot(diamonds.mod, aes(color, lprice.resid)) + geom_boxplot()




Now we see the relationship we expect: price increases with quality.

The same thing can be observed with “clarity”.


A more complicated model
Continue to build up our model, making the observed effects explicit.

mod_diamond <- lm(lprice ~ lcarat+color+cut+clarity, data=diamonds2)
This model now includes four predictors, so it’s getting harder to visualise.

Since there are no interaction terms: plot them individually in four plots.

For example, we can plot the effect of “cut”, with others fixed.

grid <- diamonds2 %>%
  data_grid(cut, lcarat = 0, color = "G", clarity = "SI1") %>%
  add_predictions(mod_diamond)
grid
## # A tibble: 5 x 5
##   cut       lcarat color clarity  pred
##   <ord>      <dbl> <chr> <chr>   <dbl>
## 1 Fair           0 G     SI1      12.0
## 2 Good           0 G     SI1      12.1
## 3 Very Good      0 G     SI1      12.1
## 4 Premium        0 G     SI1      12.2
## 5 Ideal          0 G     SI1      12.2

Plotting effects in a more complicated model
Now we can plot the effect of each variable separately.





For the effect of color, we used:

grid <- diamonds2 %>%
  data_grid(color, lcarat = 0, cut = "Good", clarity = "SI1") %>%
  add_predictions(mod_diamond)
ggplot(grid, aes(color, pred)) + geom_point()


Residuals in the more complicated model
Finally, we look at the residuals vs. carat, in the model with 4 predictors.

diamonds.mod <- diamonds2 %>% add_residuals(mod_diamond, "lresid")

ggplot(diamonds.mod) + geom_point(aes(x=lcarat, y=lresid), alpha=0.05)


Now we see no obvious trend in the residuals.

However, some diamonds have quite large residuals.




# Linear Regression

## Linear Regression

</br>

> - Regression is a supervised learning method, whose goal is inferring the 
relationship between input data, $x$, and a **continuous** response variable, $y$.

> - **Linear regression** is a type of regression where **$y$ is modeled as a 
linear function of $x$**. 

> - **Simple linear regression** predicts the output $y$ from a single predictor 
$x$. 
\[y = \beta_0 + \beta_1 x + \epsilon\]

> - **Multiple linear regression** assumes $y$ relies on many covariates:
\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon =
\vec \beta \vec x + \epsilon\]

> - here $\epsilon$ denotes a random noise term with zero mean.

## Objective function

Linear regression seeks a solution $\hat y = \hat \beta \cdot \vec x$ 
that **minimizes the difference between the true outcome $y$ and the 
prediction $\hat y$**, in terms of the residual sum of squares (RSS).

$$arg \min\limits_{\hat \beta} \sum_i \left(y^{(i)} - \hat \beta x^{(i)}\right)^2$$


## Simple Linear Regression

* Predict the mileage per gallon using the weight of the car.

* In R the linear models can be fit with a `lm()` function.

```{r}
fit <- lm(mpg ~ wt, mtcars)
fit
```

* Same '~' formula notation as for the `t.test` function. 

## 

We can check the details on the fitted model by calling:
```{r}
summary(fit)
```

##

The coefficients ($\beta$) of the model:
```{r}
(beta <-  coef(summary(fit)))
```

$\hat y$  = predicted `mpg` values for existing observations (cars): 
```{r}
predict(fit)[1:15]
```

##

To predict the `mpg` for **new observations**, e.g. a new car with 
weight `wt = 3.14`, we can do a manual computation using estimated coefficients:

```{r}
beta[, 1]
# beta0 + beta1 * wt
beta[1, 1] + beta[2, 1]* 3.14
```

Predictions can be more efficiently computed using `predict()` function: 

```{r}
# create a data frame with new weights:
newcars <- data.frame(wt = c(2, 2.1, 3.14, 4.1, 4.3))
predict(fit, newcars)
# Note the same prediction for `wt = 3.14` as computed previously.
```

## Fitting linear models
A linear model in general has the form

y=a0+a1×x1+a2×x2+...+an×xn

In R, the function for fitting linear models is lm().

The model family is specified with a formula:

lm(y~x)
y=a0+a1×x

This is how you call it in our example:

sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
## (Intercept)           x 
##    4.220822    2.051533

We will not go into the details of how models are fitted by R.



The best model (blue) minimizes the distance from the data.

‖ê ‖22=‖y−â 0−â 1×x‖22

Residuals (red):
ê =y−â 0−â 1×x

Making predictions
First, generate a grid of values covering the region where our data lies.

grid <- sim1 %>% data_grid(x) 
This finds unique values for each variables and generates all combinations.

Now make predictions (using modelr) and add them to the grid data frame.

grid <- grid %>% add_predictions(sim1_mod)
grid

Visualizing the predictions
Now we can compare our predictions to the observed values.

ggplot(sim1, aes(x)) + geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 1)
  
  
Visualizing the residuals
Add residuals to data with add_residuals() from package modelr, which works like add_predictions().

sim1 <- sim1 %>% add_residuals(sim1_mod)
We can recreate the plot using the residuals instead of y.

ggplot(sim1, aes(x, resid)) + geom_ref_line(h = 0) + geom_point() 


##

`ggplot2` with `geom_smooth()` can plot the data and the fitted `lm` model


```{r}
ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_smooth(method="lm")
```


## Multiple Linear Regression

We might like to predict `mpg` using weight, displacement and the number of 
cylinders in the car.

```{r fig.height=5.5, fig.width=12}
ggplot(mtcars, aes(x=wt, y=mpg, col=cyl, size=disp)) + geom_point()
```

##

```{r}
mfit <- lm(mpg ~ wt + disp + cyl, data = mtcars)

# Summarize the results 
summary(mfit)
```


##

To **predict `mpg` for new cars**, you must first create a data frame 
describing the attributes of the new cars:
```{r}
(newcars <- data.frame(wt = c(2, 2.1, 3.14, 4.1, 4.3),
                      disp = c(100, 200, 500,300, 210),
                      cyl = c(6,6,4,6,8)))
```

Then you can compute the predicted `mpg`

```{r}
predict(mfit, newcars)
```


## Interaction terms

* An interaction occurs when **an independent variable has a different
effect on the outcome depending on the values of another independent**. 
variable.
* For example, one variable, $x_1$ might have a different effect on $y$ within 
different categories or groups, given by variable $x_2$.

* If you are not familiar with the concept of the interaction terms,
read [this](http://www.medicine.mcgill.ca/epidemiology/joseph/courses/EPIB-621/interaction.pdf).


##

Models with **interaction effects** can be specified with '*':
```{r}
mfit_inter <- lm(mpg ~ am * wt, mtcars)
summary(mfit_inter)
```

## 

Note that '*' generates the interaction terms:
```{r}
mfit_inter <- lm(mpg ~ am * wt, mtcars)

names(coefficients(mfit_inter))
```

## 

You can also specify explicitly which terms you want:

```{r}
mfit_iter2 <- lm(mpg ~ 1 + am + wt + am:wt, mtcars)
summary(mfit_iter2)
```


## Exercise

</br>

- Go to the "Lec6_Exercises.Rmd" file, which can be downloaded
from the class website under the Lecture tab.

- Complete Exercise 2.


# Lasso Regression

## Choosing a model

* Modern datasets often have "too" many variables, e.g. predict the risk 
of a disease from the single nucleotide polymorphisms (SNPs) data.
* **Issue:** $n \ll p$ i.e. no. of predictors is much larger than than the
no. of observations. 
* **Lasso regression** is especially useful for problems,
where 

> the number of available covariates is extremely large, but
only a handful of them are relevant for the prediction of the outcome.


## Lasso Regression

* Lasso regression is simply regression with $L_1$ penalty. 
* That is, it solves the problem:

\[\hat \beta ^*  = arg \min\limits_{\hat \beta} \sum_i \left(y^{(i)} 
- \hat \beta x^{(i)}\right)^2 + \lambda \|\hat \beta\|_1\]

* It turns out that the $L_1$ norm $\|\vec x\|_1 = \sum_j |x_j|$ **promotes
sparsity**.

* The solution, $\hat \beta^*$, usually has only a small number of 
non-zero coefficients. 

* The number of non-zero coefficients depends on the choice of 
the tuning parameter, $\lambda$. The higher the
$\lambda$ the fewer non-zero coefficients.


## `glmnet`

* Lasso regression is implemented in an R package `glmnet`.
* An introductory tutorial to the package can be found 
[here](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html).

```{r}
# install.packages("glmnet")
library(glmnet)
```


##

* We go back to `mtcars` datasets and use Lasso regression
to predict the `mpg` using all variables.
* Lasso will pick a subset of predictors (the ones with non-zero 
coefficents) that best predict the `mpg`.

```{r}
head(mtcars)
```

## 
```{r}
y <- mtcars[, 1]  # mileage per gallon 
x <- mtcars[, -1] # all other variables treated as predictors
x <- data.matrix(x, "matrix") # converts to NUMERIC matrix
# Choose a training set
set.seed(123)
trainIdx <- sample(1:nrow(mtcars), round(0.7 * nrow(mtcars)))
fit <- glmnet(x[trainIdx, ], y[trainIdx])
names(fit)
```

##

* `glmnet()` compute the Lasso regression for a sequence of 
different tuning parameters, $\lambda$. 
* Each row of `print(fit)` corresponds to a particular
$\lambda$ in the sequence.
* column `Df` denotes the number of non-zero coefficients
(degrees of freedom), 
* `%Dev` is the percentage variance explained, 
* `Lambda` is the value of the currently chosen tuning parameter. 

##

```{r fig.height=3.5, fig.width=5}
# label = TRUE makes the plot annotate the curves with the corresponding coeffients labels.
plot(fit, label = TRUE) 
```

* the y-axis corresponds the value of the coefficients.
* the x-axis is denoted "$L_1$ norm" but is scaled to indicate
the number of non-zero coefficients (the effective degrees 
of freedom).

##

* Each curve corresponds to a single variable, and shows the value
of the coefficient as the tuning parameter varies.
* $\|\hat \beta\|_{L_1}$ increases and $\lambda$
decreases from left to right.
* When $\lambda$ is small (right) there are more non-zero coefficients.


The computed Lasso coefficient for a particular choice of $\lambda$ can be
printed using:

```{r}
# Lambda = 1
coef(fit, s = 1)
```

##

* Like for `lm()`, we can use a function `predict()` to 
predict the `mpg` for the training or the test data. 
* However, we need specify the value of $\lambda$ using
the argument `s`.

```{r}
# Predict for the test set:
predict(fit, newx = x[-trainIdx, ], s = c(0.5, 1.5, 2))
```

Each of the columns corresponds to a choice of $\lambda$.

## Choosing $\lambda$

* To choose $\lambda$ can use [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)).
* Use `cv.glmnet()` function to perform a k-fold cross validation.

> In k-fold cross-validation, the original sample is randomly partitioned into 
k equal sized subsamples. Of the k subsamples, a single subsample is retained 
as the validation data for testing the model, and the remaining k − 1 
subsamples are used as training data. ^[https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation]


##

```{r fig.width=5, fig.height=3.5}
set.seed(1)
# `nfolds` argument sets the number of folds (k).
cvfit <- cv.glmnet(x[trainIdx, ], y[trainIdx], nfolds = 5)
plot(cvfit)
```

* The <span style="color:red">red dots</span> are the average MSE over the k-folds.
* The two chosen $\lambda$ values are the one with $MSE_{min}$ and 
one with $MSE_{min} + sd_{min}$

## 

$\lambda$ with minimum MSE:
```{r}
cvfit$lambda.min
```


The biggest $\lambda$ such that the MSE is within one standard error 
of the minimum MSE.
```{r}
cvfit$lambda.1se
```


## Extra Exercise

In this exercise you will perform Lasso regression yourself.
We will use the `Boston` dataset from the `MASS` package.
The dataset contains information on the Boston suburbs 
housing market collected by David Harrison in 1978.

We will try to predict the median value of of homes in the region based on 
its attributes recorded in other variables.

First install the package:
```{r}
# install.packages("MASS")
library(MASS)
```

##
```{r}
head(Boston, 3)
str(Boston)
```

##

Split the data to training and testing subsets.
```{r}
set.seed(123)
trainIdx <- sample(1:nrow(Boston), round(0.7 * nrow(Boston)))
boston.test <- Boston[-trainIdx,"medv"]
```


Perform a Lasso regression with `glmnet`. Steps:
  
1. Extract the input and output data from the `Boston` `data.frame` and convert
them if necessary to a correct format.
2. Use cross-validation to select the value for $\lambda$.
3. Inspect comuted coefficients for `lambda.min`.
4. Compute the predictions for the test dataset the two choices of the tuning
parameter, `lambda.min` and `lambda.1se`. 
Evaluate the MSE for each.























