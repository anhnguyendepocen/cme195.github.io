---
title: "Lecture 6: Exercises with answers"
date: October 19th, 2017
output: 
  html_notebook:
    toc: true
    toc_float: true
---


```{r}
library(tidyverse)
```



# Exercise 1: Linear Regression

## Part 1

The data in the following URL "http://www-bcf.usc.edu/~gareth/ISL/Income1.csv"
includes observation on income levels (in tens of thousands of dollars)
and years of education. *The data is not real and was actually simulated*.

a. Read the data into R.
```{r}
income <- read.csv("http://www-bcf.usc.edu/~gareth/ISL/Income1.csv", row.names = 1)
income
```
b. Generate a ggplot with a fitted line.

```{r}
library(ggplot2)
ggplot(income, aes(x = Education, y = Income)) +
  geom_point() + geom_smooth(method = "lm")
```


c. Fit a linear model with education as input variable and income as response 
variable. Then, print the model summary

```{r}
fit <- lm(data = income, formula = Income ~ Education)
summary(fit)
```


d. Print out just the model coefficients.

```{r}
coef(fit)
```

d. Print out the predicted values of income for the observations
included in the dataset.

```{r}
predict(fit)
```


e. Predict the income for new observations, for people with 
16.00, 12.52, 15.55, 21.09, and 18.36 years of education.

```{r}
predict(fit, data.frame(Education = c(16.00, 12.52, 15.55, 21.09, 18.36)))
```


## Part 2

a. Now download data from "http://www-bcf.usc.edu/~gareth/ISL/Income2.csv"
which include the same observations but also records data on "senority".

```{r}
income2 <- read.csv("http://www-bcf.usc.edu/~gareth/ISL/Income2.csv", row.names = 1)
income2
```

b. Fit a new model including a new variable and print the model summary.

```{r}
mfit <- lm(data = income2, formula = Income ~ Education + Seniority)
summary(mfit)
```

c. Print the predicted values of income for the existing observations.

```{r}
predict(mfit)
```


d. Predict the income levels for new observations with years of education
equal to 16.00, 12.52, 15.55, 21.09, 18.36 and seniority to
123.74, 83.63,  90.94, 178.96, 125.17.

```{r}
newobs <- data.frame(Education = c(16.00, 12.52, 15.55, 21.09, 18.36),
                     Seniority = c(123.74, 83.63,  90.94, 178.96, 125.17))
predict(mfit, newobs)
```


## Exercise 2

In this exercise you will perform Lasso regression yourself.
We will use the `Boston` dataset from the `MASS` package.
The dataset contains information on the Boston suburbs 
housing market collected by David Harrison in 1978.

We will try to predict the median value of of homes in the region based on 
its attributes recorded in other variables.

First install the package:
```{r}
# install.packages("MASS")
library(MASS)
```

```{r}
head(Boston, 3)
str(Boston)
```


a. Split the data to training and testing subsets.

```{r}
set.seed(123)
trainIdx <- sample(1:nrow(Boston), round(0.7 * nrow(Boston)))
boston.test <- Boston[-trainIdx,"medv"]
```


b. Perform a Lasso regression with `glmnet`. Steps:
  
1. Extract the input and output data from the `Boston` `data.frame` and convert
them if necessary to a correct format.
2. Use cross-validation to select the value for $\lambda$.
3. Inspect computed coefficients for `lambda.min`.
4. Compute the predictions for the test dataset the two choices of the tuning
parameter, `lambda.min` and `lambda.1se`. 
Evaluate the MSE for each.

